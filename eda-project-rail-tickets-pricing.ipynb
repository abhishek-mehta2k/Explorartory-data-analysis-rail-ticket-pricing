{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Spanish Railway Ticket Pricing\n",
    "\n",
    "Data Source : [Spanish Rail Tickets Pricing](https://www.kaggle.com/datasets/thegurusteam/spanish-high-speed-rail-system-ticket-pricing)\n",
    "<br> <br>\n",
    "![Imgur](https://imgur.com/aQxs8Ho.jpg)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EDA (Exploratory Data Analysis) \n",
    "\n",
    ">### What is EDA?\n",
    "**EDA (Exploratory Data Analysis)** is the process through which we extract data from a website, and save it in a form which is easy to read, to understand and to work on. \n",
    "\n",
    ">When we say 'Easy to work on', we mean to say that the data thus extracted can be used to get a lot of useful insights and answer a lot of questions, finding answers to which would not be such an easy task, if we did not have that data stored with us in a simple and sorted manner, i.e. generally in an ` Excel File or a CSV file`.\n",
    "\n",
    ">Exploratory Data Analysis, or EDA, is an important step in any Data Analysis or Data Science project. EDA is the process of investigating the dataset to discover patterns, and anomalies (outliers), and form hypotheses based on our understanding of the dataset.\n",
    "\n",
    ">  EDA is basically used to see what data can reveal beyond the formal modelling or hypothesis testing task and provides better understanding of data set variables and the relationship between them. Originally developed by American mathematician Jhon Tukey in the 1970s, EDA techniques continue to be a widely used method in the data discovery process today.\n",
    "\n",
    ">EDA can help us deliver great business results, by improving our existing knowledge and can also help in giving out new insights that we might not be aware of\n",
    "![](https)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tools Used**\n",
    "\n",
    "* `opendatasets` (Jovian library to download a Kaggle dataset) \n",
    "<br>\n",
    "\n",
    "* Data cleaning:\n",
    "  \n",
    "  1.`Pandas`\n",
    "  \n",
    "  2.`Numpy`\n",
    "<br>\n",
    "\n",
    "* Data Visualization\n",
    "  \n",
    "  1.`Matplotlib` \n",
    "  \n",
    "  2.`Seaborn`\n",
    "  \n",
    "  3.`plotly`\n",
    "  \n",
    "  4.`folium`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Project\n",
    "In this project, we are trying to analyse Spain Rail Ticket Data. This selected dataset covers data about the different trips by different trains with differnt duration and prices. \n",
    "Personally I find `Pricing` quite interesting because it helps to analyze about pattern of the change in price over time period and many factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps followed\n",
    "\n",
    "### Step 1: Selecting a real world dataset:\n",
    "* We will download our dataset from `Kaggle`   using the library `opendataset` created by `Jovian` which imports the datasets directly from the 'kaggle' website\n",
    "\n",
    "  import opendatasets as od\n",
    "\n",
    "  dataset = 'https://www.kaggle.com/datasets/thegurusteam/spanish-high-speed-rail-system-ticket-pricing'\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Performing data preperation & cleaning\n",
    "* We will load the dataset into a dataframe  using Pandas, explore the different columns and range of values, handle missing values and incorrect datatypes and basically make our data ready to use for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:Perform exploratory analysis and visualization and asking interesting questions\n",
    " * We will compute the mean, sum, range and other interesting statistics for numeric columns, explore distributions of numeric columns using histogram etc, make a note of interesting insights from the exploratory analysis, ask interesting questions about the dataset and look for their answers through visualizing our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Happy Coding!!**\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Install packages and import libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below cell, to install all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install jovian --upgrade --quiet\n",
    "# !pip install scipy pandas seaborn numpy matplotlib wordcloud  --upgrade --quiet\n",
    "# !pip install opendatasets plotly folium --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library to download data from Kaggle\n",
    "import opendatasets as od\n",
    "\n",
    "# Import python data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize      #---- Handles outliers\n",
    "\n",
    "# Import visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Import other required libraries\n",
    "import jovian\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set a base style for all our visualisations in this notebook. These customisations from [Mounir](https://jovian.ai/kara-mounir) create excellent clarity images to use in presentations. You can override them and customise individual plots as  required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot parameters for the notebook\n",
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = (18, 10)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'\n",
    "matplotlib.rcParams['xtick.major.pad']='10'\n",
    "matplotlib.rcParams['ytick.major.pad']='10'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"zerotoanalyst-rail-ticket-pricing-eda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Downloading the Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Working with large datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To download dataset from kaggle we have to provide an `API Key` which can done by following two methods**\n",
    ">1. By entering the `API Key` manually at the runtime.\n",
    ">2. By providing the `API Key` by a file `kaggle.json`, which automatically does the authentication for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/datasets/thegurusteam/spanish-high-speed-rail-system-ticket-pricing?select=thegurus-opendata-renfe-trips.csv'\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the data has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './spanish-high-speed-rail-system-ticket-pricing'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been downloaded and unzipped to the folder `./spanish-high-speed-rail-system-ticket-pricing` Let us now check the size of the folder. There is one file in this folder.  \n",
    "`thegurus-opendata-renfe-trips.csv` 7.24 GB\n",
    "> We will use this file for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There are 38.75 million records in thegurus-opendata-renfe-trips csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#load the file using Pandas. \n",
    "rail_ticket_price_df = pd.read_csv(data_dir+'/thegurus-opendata-renfe-trips.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phenomenal!! That's 7.24 GB data over 38.75 million records loaded into a Pandas dataframe in under  8 mins!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_ticket_price_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 38.75 million rows and 13 columns in the dataset. Using Pandas this dataset is now 4 GB  from 7.24 GB as a `.csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Work with a sample - a fraction of the dataset**\n",
    "\n",
    "As this massive dataset let's use 1% of the data to build our EDA framework. This is very important and often overlooked. Working on a smaller sample saves significant time while experimenting with code.\n",
    "\n",
    "Once we have the full framework ready, we can run the analysis on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rail_ticket_price_sample_df = rail_ticket_price_df.sample(frac=0.01)\n",
    "rail_ticket_price_sample_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample dataset now contains 387k rows and the same 13 columns. It is 44.3 MB in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save intermediate results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with large datasets, you may runtinto runtime issues. Therefore it will be helpful to save the intermediate results to your google drive or a local folder to pick up and continue from the point.  \n",
    "\n",
    "Let us save this sample dataset on our local machine. We will use the data in this file to build our framework.\n",
    "\n",
    "We will continue to save short snapshots of data offline as we move along the notebook. You can save the data in various formats including binary formats such as `.feather`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to CSV file.\n",
    "rail_ticket_price_sample_df.to_csv('rail_ticket_price_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explore faster loading and lesser memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some techniques to load data to Pandas faster and use less memory.\n",
    "\n",
    "- **drop columns:** select a subset of columns relevant for analysis\n",
    "- **identify categorical columns:** change the dtype tp `category`\n",
    "- **parse_dates:** change columns with date\\time to type `DateTime`\n",
    "- set **DateTime** column as the **index**\n",
    "- **use smaller dtypes**: we don't see any need as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only a subset of the columns\n",
    "selected_cols = ['origin', 'destination', 'departure', 'arrival', 'duration',\n",
    "                 'vehicle_type', 'vehicle_class', 'price', 'fare']\n",
    "\n",
    "selected_dtypes = {\n",
    "    'duration': 'float32',\n",
    "    'price': 'float32',   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remove comment in the cell below to run the analysis on the appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample data\n",
    "# sample_csv_url = './rail_ticket_price_sample.csv'\n",
    "\n",
    "#Full data\n",
    "sample_csv_url = './spanish-high-speed-rail-system-ticket-pricing/thegurus-opendata-renfe-trips.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ticket_price_df = pd.read_csv(sample_csv_url, \n",
    "                            usecols=selected_cols, \n",
    "                            dtype=selected_dtypes, \n",
    "                            parse_dates=['departure', 'arrival'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(ticket_price_df,\n",
    "          x = 'price',\n",
    "          y = 'vehicle_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting only `required columns` and defining `datatypes` before reading,\n",
    "The total time dropped from `11min 18sec` to `4min 19sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Dataset now have around `38.75 million` of records and we have selected `9` Features out of `13` to perform our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Work\n",
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data pre-processing**\n",
    "\n",
    "Now that we have loaded the data into a Pandas dataframe, let us process the data for the following\n",
    "\n",
    "- drop duplicates\n",
    "- replace missing values\n",
    "- check for outliers\n",
    "\n",
    "You could analyse the data more to further clean up the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Drop duplicates**\n",
    "\n",
    "Print number of unique values for each columns before checking for duplicates. These are fairly time consuming when we run them on the complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save duplicates for analysis\n",
    "duplicates_df = ticket_price_df[ticket_price_df.duplicated(['origin','destination','departure','arrival','duration','vehicle_type','vehicle_class','price','fare'])]\n",
    "len(duplicates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found 38,343,380 duplicate entries\n",
    ">by this we can see that how many duplicate entries can be present in a dataset, and the importance of `Data Cleaning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates by departure and arrival for further analysis\n",
    "grp_df = (duplicates_df.groupby( ['origin','destination','departure','arrival','duration','vehicle_type','vehicle_class','price','fare'])[[ \"departure\", \"arrival\"]].size()\n",
    "                      .reset_index(name='group_count')\n",
    "                      .sort_values(by= 'group_count',ascending= False))\n",
    "#Look at the highest number of duplicates.\n",
    "grp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate rows\n",
    "ticket_price_df.drop_duplicates(keep=False, inplace= True)\n",
    "\n",
    "#check for missing data\n",
    "missing_data_pct = ticket_price_df.isna().sum().sort_values(ascending=False)/ticket_price_df.shape[0]\n",
    "missing_data_pct*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if we still have any duplicates\n",
    "ticket_price_df.drop_duplicates().duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check for outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "ticket_price_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking as the values of percentiles we can find out that we have outliers in `duration` and `price` column.\n",
    "We will view the outliers and remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling outliers of `duration` column using `Winsorize Method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for duration column to spot the outliers\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "sns.boxplot(x = ticket_price_df['duration'])\n",
    "plt.title('Total Duration of trip')\n",
    "plt.xlabel('Total time (Hrs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticket_price_df.duration.quantile(0.01))\n",
    "print(ticket_price_df.duration.quantile(0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['duration'] = winsorize(ticket_price_df['duration'], (0.01, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for duration column to spot the outliers\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "sns.boxplot(x = ticket_price_df['duration'])\n",
    "plt.title('Total Duration of trip')\n",
    "plt.xlabel('Total time (Hrs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling outliers of `price` column using `Q3 - Q1` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Price column to spot the outliers\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "sns.boxplot(x = ticket_price_df['price'])\n",
    "plt.title('Total Cost of trip')\n",
    "plt.xlabel('Total Price (Euro)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3-Q1-->73.85-37.80-->36.05/2-->18.025\n",
    "ticket_price_df.drop(ticket_price_df[ticket_price_df.price < (37.80-18.025)].index, inplace=True)\n",
    "ticket_price_df.drop(ticket_price_df[ticket_price_df.price > (73.85+18.025)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Price column to spot the outliers\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "sns.boxplot(x = ticket_price_df['price'])\n",
    "plt.title('Total Cost of trip')\n",
    "plt.xlabel('Total Price (Euro)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Find and replace missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket_price_df.to_csv('price_null.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket_price_df['price'].fillna(ticket_price_df['price'].mean(), inplace = True)\n",
    "# price_null_df = pd.read_csv('price_null.csv')\n",
    "ticket_price_df['price'] = ticket_price_df['price'].interpolate()\n",
    "ticket_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['vehicle_class'].fillna(ticket_price_df['vehicle_class'].mode()[0], inplace = True)\n",
    "ticket_price_df['fare'].fillna(ticket_price_df['fare'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['figure.figsize'] = (9,5)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Cost for travelling by different types of Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type_list = ticket_price_df['vehicle_type'].value_counts().index.tolist()\n",
    "vehicle_type_dict = {'vehicle_type':[],\n",
    "                     'price':[]}\n",
    "for vehicle_type in vehicle_type_list:\n",
    "    average_ticket_price = (ticket_price_df.loc[ticket_price_df['vehicle_type'] == vehicle_type, 'price'].mean())\n",
    "    vehicle_type_dict['vehicle_type'].append(vehicle_type)\n",
    "    vehicle_type_dict['price'].append(average_ticket_price)\n",
    "\n",
    "avg_price_df = pd.DataFrame.from_dict(vehicle_type_dict)\n",
    "# avg_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.xticks(rotation = 75)\n",
    "plt.title('Cost of travelling by different Train Types')\n",
    "sns.barplot(x=avg_price_df.vehicle_type, y = avg_price_df.price)\n",
    "plt.xlabel('Type of Train')\n",
    "plt.ylabel('Cost of travelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the `Cost of travelling` by `AVE` lies in top 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passengers travelling by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = ticket_price_df['vehicle_type'].value_counts()\n",
    "# vehicle_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "sns.barplot(x=vehicle_type.index, y = vehicle_type)\n",
    "plt.ylabel('No. of Trips')\n",
    "plt.xticks(rotation = 75)\n",
    "plt.xlabel('Type of Train')\n",
    "plt.title('No. of Passengers Travelling by (Train Types)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It seems that disproportionately high number of passenger are travelling from the` Alta Velocidad Española (AVE)` which travells at a speed of `310 km/h`. Even thou the cost of travelling by `AVE` is quite high (lies in `top 10%`) ,we can clearly see here that most of the passengers like to travell by high-speed-train as it takes less time to reach to the destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adding `month` column to see trends over month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticket_price_df['month']=ticket_price_df['departure'].dt.month_name()\n",
    "ticket_price_df['month_no']=ticket_price_df['departure'].dt.month\n",
    "ticket_price_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average duration for which train runs, over different months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_duration_df = ticket_price_df[['month_no','month','duration']].groupby(['month_no','month']).mean()\n",
    "month_duration_df.reset_index(inplace = True)\n",
    "matplotlib.rcParams['figure.figsize'] = (16,6)\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Average Travelling Time (Hrs)')\n",
    "plt.title('Average Duration of Travelling over different Months')\n",
    "plt.xticks(rotation = 45)\n",
    "sns.lineplot(data = month_duration_df, x=\"month\", y = 'duration',color = 'green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see that the Train's average running time is quite high during the month of `April` and `May` and very low in the month of `Feburary`, `June` and `September` , so we can `reduce` the number of trains when the average running time of trians are very low i.e during the month of `Feburary` and `October`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Earning from Train, throughout the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_price_df = ticket_price_df[['month_no','month','price']].groupby(['month_no','month']).sum()\n",
    "month_price_df.reset_index(inplace = True)\n",
    "matplotlib.rcParams['figure.figsize'] = (16,6)\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Average Travelling Cost (Euro) ')\n",
    "plt.title('Total Earning from Railways thoughout the year')\n",
    "plt.xticks(rotation = 45)\n",
    "sns.lineplot(data = month_price_df, x=\"month\", y = 'price',color = 'orange')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Earnings of Government by Railways in Spain is high during the month of `March` and very low for the month of `June`, `September`, `December` and `January`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating geographical co-ordinates data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(ticket_price_df['destination'].unique())\n",
    "temp_df.columns = ['city']\n",
    "location_df = pd.read_csv('es.csv', encoding= 'unicode_escape')\n",
    "geo_data_df = pd.merge(temp_df, location_df, how='outer', on = 'city')\n",
    "geo_data_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added No. of trips started from particular city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ticket_price_df['origin'].value_counts())\n",
    "df.reset_index(inplace = True)\n",
    "df.columns = ['city','count']\n",
    "geo_data_origin_df = pd.merge(df,geo_data_df, how='left', on='city')\n",
    "# geo_data_origin_df.sample(5)\n",
    "geo_data_origin_df.sort_values(by='count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geo_data_origin_df\n",
    "m = folium.Map(location = [40.4637, 1], tiles ='OpenStreetMap',    \n",
    "    zoom_start=6)\n",
    "for i, row in df.iterrows():\n",
    "    lat = df.at[i, 'lat']\n",
    "    lng = df.at[i, 'lng']\n",
    "    \n",
    "    iframe = str('''<u><h4> City :</h4></u>'''+ df.at[i, 'city']  +''' <br> <u><h4> No.of Tirps Started :</h4></u>''' + str(df.iloc[i]['count'])) \n",
    "    popup = folium.Popup(iframe,\n",
    "                     max_width=200)\n",
    "    folium.Marker(location = [lat, lng], popup= popup,fill_color='#43d9de',icon=plugins.BeautifyIcon(\n",
    "                         icon=\"arrow-down\", icon_shape=\"marker\",\n",
    "                         number=str(df.iloc[i]['count']),\n",
    "                         border_color= 'grey',\n",
    "                         background_color= '#43d9de')).add_to(m)\n",
    "# m.save('trip_origin.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that maximum number of trips started from `Madrid` which lies in the center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added No. of trips ended to particular city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ticket_price_df['destination'].value_counts())\n",
    "df.reset_index(inplace = True)\n",
    "df.columns = ['city','count']\n",
    "geo_data__dest_df = pd.merge(df,geo_data_df, how='left', on='city')\n",
    "# geo_data__dest_df.sample(5)\n",
    "geo_data__dest_df.sort_values(by='count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geo_data__dest_df\n",
    "m = folium.Map(location = [40.4637, 1], tiles ='OpenStreetMap',    \n",
    "    zoom_start=6)\n",
    "for i, row in df.iterrows():\n",
    "    lat = df.at[i, 'lat']\n",
    "    lng = df.at[i, 'lng']\n",
    "    \n",
    "    iframe = str('''<u><h4> City :</h4></u>'''+ df.at[i, 'city']  +''' <br> <u><h4> Destination of Tirps :</h4></u>''' + str(df.iloc[i]['count'])) \n",
    "    popup = folium.Popup(iframe,\n",
    "                     max_width=200)\n",
    "    folium.Marker(location = [lat, lng], popup= popup,fill_color='#43d9de',icon=plugins.BeautifyIcon(\n",
    "                         icon=\"arrow-down\", icon_shape=\"marker\",\n",
    "                         number=str(df.iloc[i]['count']),\n",
    "                         border_color= 'grey',\n",
    "                         background_color= '#43d9de')).add_to(m)\n",
    "# m.save('trip_origin.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that maximum number of trips ended on `Madrid` which lies in the center and is connected to all the cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating two base categories `Standard Tourist Class` & `First Class` for vehicle class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df = ticket_price_df.replace({'vehicle_class':{'Turista':'Tourist standard class','Preferente':'First class','Turista con enlace':'Tourist standard class',\n",
    "                                                            'Turista Plus':'Tourist standard class','TuristaSólo plaza H':'Tourist standard class',\n",
    "                                                            'Turista Plus - Turista': 'Tourist standard class', 'PreferenteSólo plaza H':'First class'\n",
    "                                         }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Creating `Buckets` for duration of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['duration_b'] = pd.DataFrame(pd.cut(ticket_price_df['duration'],8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20,10)\n",
    "tourist_class_df1 = ticket_price_df[ticket_price_df['vehicle_class'] == 'Tourist standard class']\n",
    "sns.stripplot(data=tourist_class_df1, x=\"duration_b\", y=\"price\")\n",
    "plt.xlabel('Duration (Hrs)')\n",
    "plt.ylabel('Price (Euro)')\n",
    "plt.title('Duration Vs Price (Standard Class)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people travelling by `Tourist Standard Class` have a travelling time between `1 to 7 hrs` and over different train types the price for the same duration varies alot i.e from `€20` to `€90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20,10)\n",
    "tourist_class_df1 = ticket_price_df[ticket_price_df['vehicle_class'] == 'First class']\n",
    "sns.stripplot(data=tourist_class_df1, x=\"duration_b\", y=\"price\")\n",
    "plt.xlabel('Duration (Hrs)')\n",
    "plt.ylabel('Price (Euro)')\n",
    "plt.title('Duration Vs Price (Standard Class)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `First class` very few people travell for a duration longer than`3.3 hrs`, and most of the people travell from `1 hrs` to `2.5 hrs` and `2.5 hrs` to `3.3 hrs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['weekday'] = ticket_price_df['departure'].dt.day_name()\n",
    "ticket_price_df['weekday_no'] = ticket_price_df['departure'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['hour'] = ticket_price_df['departure'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = (ticket_price_df[['duration','vehicle_type','price']]).round(0)\n",
    "# trains = trains[trains['price'].notna()]\n",
    "df_heatmap = trains.pivot_table(values='price',index='vehicle_type',columns='duration')\n",
    "sns.heatmap(df_heatmap, linewidths = .5, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the price for some trains are quite high even for the shorter duration and that is beacause of train's feature\n",
    "> but the most expensive train is `AVE-TGV` which have a maximum trip time of `4 hrs` \n",
    "> and for the duration of `7 hrs` the minimum cost is of train `AVE-LD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['duration'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most number of trips between..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_price_df['origin_destination'] = ticket_price_df[['origin', 'destination']].agg('_'.join, axis=1)\n",
    "brand_display_text= ticket_price_df.origin_destination.str.cat(sep=\" \")\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=800, height=600, margin=0, background_color='white').generate(brand_display_text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary: Spain Rail Ticket Pricing** \n",
    "\n",
    "We analysed the behaviour of Railway ticket pricing using Python, Pandas, Matplotlib and Seaborn. Here is a summary of the key insights for the Dataset.\n",
    "\n",
    "Key Metrics:\n",
    "\n",
    "- Even thou the price of travelling by `AVE` train is quite high the people travell alot by it so we can increase the number of `AVE` trains\n",
    "\n",
    "- Maximum amount of earning is during the month of `March`, and very low during the month of `January`, `June`, `September` and `December`, so we can decrease the number of train during the months where earning is low and increase for the month of `January`\n",
    "\n",
    "- Very few people travel more than `3.3 hrs` while travelling by First class, so we can reduce the number of First class and increase the Standard Class in the long journey train trips.\n",
    "\n",
    "We also discovered the following insights from our exploratory data analysis\n",
    "- For the long distance travelling the train `AVE-LD` is pocket friendly(low in cost) and we have many passengers travelling by this train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Jovian tutorials\n",
    "  - [Analyzing Tabular Data with Pandas](https://jovian.ai/aakashns/python-pandas-data-analysis)\n",
    "  - [Data Visualization using Python, Matplotlib and Seaborn](https://jovian.ai/aakashns/python-matplotlib-data-visualization)\n",
    "  - [Advanced Data Analysis Techniques with Python & Pandas](https://jovian.ai/aakashns/advanced-data-analysis-pandas)\n",
    "  - [Exploratory Data Analysis Case Study - Stack Overflow Developer Survey](https://jovian.ai/aakashns/stackoverflow-survey-exploratory-data-analysis)\n",
    "  -  [Interactive Visualization with Plotly](https://jovian.ai/aakashns/interactive-visualization-plotly)\n",
    "- [10 Key Metrics You MUST Know When Working with Web Data](https://www.youtube.com/watch?v=ZO-YwkVk8Vo) by Eric Sims\n",
    "- EDA - [Kaggle code](https://www.kaggle.com/datasets/thegurusteam/spanish-high-speed-rail-system-ticket-pricing) by THE GURUS\n",
    "- [Stackoverflow](https://stackoverflow.com/) hacks, links throughout the notebook\n",
    "- [Geeks for Geeks](https://www.geeksforgeeks.org/)\n",
    "- [Medium](https://python.plainenglish.io/using-folium-to-map-latitude-and-longitude-491f8dcc81ad/)\n",
    "- [Seborn](https://seaborn.pydata.org/)\n",
    "- [Pandas](https://pandas.pydata.org/docs/)\n",
    "- [Handling Missing Values](https://towardsdatascience.com/data-cleaning-how-to-handle-missing-values-in-pandas-cc8570c446ec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save our work\n",
    "jovian.commit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}